## Отчёт по заданию 3 «GenAI»: BPE и GPT


**Ноутбук с кодом и комментариями:**

[Открыть в Kaggle](https://www.kaggle.com/code/messermmp/mts-hw3-panasiuk)

---


### 1. Выбор корпуса

* **Датасет:** `Den4ikAI/russian_dialogues_2` (содержит диалоги в виде списков реплик).
* **Объём:** первые **1 000** диалогов

---

Вот обновлённый, более структурированный и красиво оформленный вариант твоего пункта **«2. Исследование BPE-токенизации»** с добавленной **оценкой словарей** и **тезисным анализом пересечений**:

---

### 2. Исследование BPE-токенизации

Обучен BPE-токенизатор (`BPETokenizer`) на объединённых 1 000 диалогах для трёх значений числа merge-операций: `10`, `100` и `1000`.

#### Размер итоговых словарей

| Кол-во merges | Размер словаря (токенов) |
| :-----------: | :----------------------: |
|     **10**    |            166           |
|    **100**    |            256           |
|    **1000**   |           1152           |

---

#### Доля пересечений между словарями (коэффициент Жаккара)

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

|    Сравнение    | \|A∩B\| | \|A∪B\| | J-коэффициент |
| :-------------: | :-----: | :-----: | :-----------: |
|  **10 vs 100**  |   166   |   256   |   **0.648**   |
| **100 vs 1000** |   256   |   1152  |   **0.222**   |
|  **10 vs 1000** |   166   |   1152  |   **0.144**   |

**Интерпретация коэффициентов:**

* Большая доля пересечений между словарями 10 и 100 говорит о том, что при 100 merges используется почти та же база символов, дополненная частыми биграммами и морфемами.
* Между 100 и 1000 наблюдается сильное расширение словаря, включающее более сложные субтокены.
* Минимальное пересечение между 10 и 1000 подчёркивает, что при 1000 merges токенизация становится более семантически насыщенной и далёкой от побуквенной.

---

#### Анализ словарей

* **Словарь при 10 merges (`final_vocab_10.json`):**

  * Почти полностью состоит из отдельных символов, включая кириллицу, знаки препинания, эмодзи.
  * Представляет собой низкоуровневое побуквенное разбиение.
  * Практически не содержит семантической информации.

* **Словарь при 100 merges (`final_vocab_100.json`):**

  * Включает частотные биграммы и триграммы (например, `что`, `про`, `при`, `раз`, `ого`).
  * Начинают появляться отдельные морфемы, приставки, окончания.
  * Лучше отражает структуру русского языка.

* **Словарь при 1000 merges (`final_vocab_1000.json`):**

  * Богатый набор субтокенов, включая целые слова и устойчивые выражения (`пожалуйста`, `спасибо`, `ладно`, `разве`, `поэтому`).
  * Хорошее покрытие ключевых лексических единиц.
  * Максимальная семантическая плотность и эффективность при токенизации.

---

#### Выводы

* С увеличением числа merge-операций:

  * **Токены становятся семантически осмысленнее**: от символов → морфемы → слова.
  * **Словари приобретают больше уникальных и информативных единиц**, что снижает среднюю длину токенизированного текста.
  * **Покрытие языка существенно улучшается**, особенно заметно при переходе от 100 к 1000 merges.
  * **Коэффициенты Жаккара** показывают постепенное расширение словаря с сохранением базовой части (символов), но с резким скачком разнообразия.

---


### 3. Обучение GPT-модели

* **Архитектура:**

  * Transformer-блоки с causal-mask, позиционными эмбеддингами и dropout(0.1).
  * Параметры: `d_model=512`, `num_heads=8`, `num_layers=4`.
* **Обучение:**

  * `block_size=128`, `batch_size=128`, `epochs=5`, `lr=1e-3` с CosineAnnealingLR.
  * Оптимизатор: Adam.
  * Устройство: GPU (если есть).
* **Сохранение:** модель сохранялась после каждой эпохи (`model_epoch_{i}.pt`) и в финал `final_model.pt`.

---

### 4. Примеры диалогов (inference)

Запущено по заранее заданным промптам (макс. 30 новых токенов).

**Prompt 1:** «Привет! Как твои дела?»
**Response 1:**

> При вет ! Как тво и дела ? и и . , и . , и . , и . , и ди . , и ди .

**Prompt 2:** «Расскажи веселый анекдот про программистов.»
**Response 2:**

> Ра сс каж и ве се лый ан е к до т про про гра мм исто в. Ну и про ве се тей про ве се тей про ве се тей про ве се тей про ве

**Prompt 3:** «Опиши крутую поездку в деревню.»
**Response 3:**

> О пи ши кру ту ю по ез д ку в дер ев ню . по пи сь ны пи сь ны пи сь ны пи сь ны пи сь ны пи пи пи пи

**Prompt 4:**
«Недавно я начал изучать иностранные языки и выбрал английский. Посоветуй с чего лучше начать обучение и сколько часов уделять в неделю?»
**Response 4:**

> Не давно я нача л из у ча ть ино стра нные я зы ки и вы бра л ан гли й ский . По сов ет у й с чего лучше нача ть об у че ние и сколько час о в у де ля ть в не де лю ? Мо гу ля ть в чём смы сле ку ум у ча я не у ча я не у ча

---

### 5. Оценка качества генерации

| Критерий            | Описание                                           | Оценка (1–5) |
| ------------------- | -------------------------------------------------- | -----------: |
| **Грамотность**     | Нет связного текста, слитно разбивается по токенам |            1 |
| **Релевантность**   | Ответы повторяют часть вопроса без смысла          |            1 |
| **Плавность**       | Абсолютно нет связности                            |            1 |
| **Информативность** | Почти полное отсутствие содержания                 |            1 |

> **Итоговая оценка:** модель не научилась генерировать осмысленные ответы.
---
