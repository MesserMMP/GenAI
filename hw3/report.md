## Отчёт по заданию 3 «GenAI»: BPE и GPT

**Ноутбук с кодом и комментариями:**
[Открыть в Collab](https://colab.research.google.com/drive/1eUaM3YUAus4pw3Q4mur_aarigIz_mTyW?usp=sharing)

---

### 1. Выбор корпуса

* **Датасет:** `Den4ikAI/russian_dialogues_2` (содержит диалоги в виде списков реплик).
* **Объём:** первые **1 000** диалогов объединены в одну строку и использованы для обучения токенизатора и модели.

---

### 2. Исследование BPE-токенизации

Обучен BPE-токенизатор (`BPETokenizer`) на объединённом корпусе для трёх значений количества merge-операций: `10`, `100`, `1000`.

#### Размер итоговых словарей

| Кол-во merges | Размер словаря (токенов) |
| :-----------: | :----------------------: |
|     **10**    |            166           |
|    **100**    |            256           |
|    **1000**   |           1156           |

---

#### Доля пересечений между словарями (коэффициент Жаккара)

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

|    Сравнение    | \|A∩B\| | \|A∪B\| | J-коэффициент |
| :-------------: | :-----: | :-----: | :-----------: |
|  **10 vs 100**  |   166   |   256   |   **0.648**   |
| **100 vs 1000** |   256   |   1156  |   **0.222**   |
|  **10 vs 1000** |   166   |   1156  |   **0.144**   |

---

#### Анализ словарей

* **Словарь при 10 merges (`final_vocab_10.json`):**

  * Состоит в основном из символов.
  * Очень низкий уровень обобщения.
  * Не отражает семантическую структуру.

* **Словарь при 100 merges (`final_vocab_100.json`):**

  * Содержит частотные биграммы, морфемы и окончания.
  * Лучше моделирует морфологию русского языка.

* **Словарь при 1000 merges (`final_vocab_1000.json`):**

  * Содержит слова и устойчивые выражения.
  * Семантически плотный и эффективный для генерации.

---

#### Выводы

* Увеличение количества merge-операций ведёт к:

  * Переходу от символов → морфем → слов.
  * Снижению длины токенизированного текста.
  * Повышению семантической насыщенности словаря.
  * Улучшению покрытия языка.

---

### 3. Обучение GPT-модели

* **Архитектура:**

  * Классический Transformer с causal-mask.
  * `d_model=512`, `num_heads=8`, `num_layers=4`.
  * Используются позиционные эмбеддинги и dropout 0.1.

* **Обучение:**

  * `block_size=128`, `batch_size=128`, `epochs=5`.
  * Оптимизатор Adam, learning rate: 1e-3.
  * Scheduler: CosineAnnealingLR.
  * Устройство: GPU (если доступен).

* **Сохранение:**

  * Модель сохранялась после каждой эпохи (`model_epoch_{i}.pt`) и итогово как `final_model.pt`.

---

### 4. Примеры диалогов (inference)

Модель запускалась по заранее заданным промптам (максимум 30 новых токенов):

**Prompt 1:** «Привет! Как твои дела?»
**Response 1:**

> Как там там там там там там там там там там там там там там там там там там там

---

**Prompt 2:** «Расскажи весёлый анекдот про программистов.»

**Response 2:**

> В про ве се лё и ве се ного про ве се ц води т бес при ве ша ется

---

**Prompt 3:** «Опиши крутую поездку в деревню.»

**Response 3:**

> Хо ло дви га ту по ку по ва ных по ва чный Ма ло с ю Да ю по

---

**Prompt 4:**
«Недавно я начал изучать иностранные языки и выбрал английский. Посоветуй, с чего лучше начать обучение и сколько часов уделять в неделю?»

**Response 4:**

> Во сс кие С у , вы я не знаю , я не стоит Если у , я не у

---

### 5. Оценка качества генерации

| Критерий            | Описание                                                                                                        | Оценка (1–5) |
| ------------------- | --------------------------------------------------------------------------------------------------------------- | ------------ |
| **Грамотность**     | Много синтаксических и морфологических ошибок, бессвязные фразы, но хотя бы ответы начинаются с заглавных букв. | 2            |
| **Релевантность**   | Ответы формально связаны с запросом, но не содержат осмысленного содержания.                                    | 1            |
| **Плавность**       | Высокая повторяемость фраз, неестественная структура.                                                           | 1            |
| **Информативность** | Почти полное отсутствие новых или полезных фактов.                                                              | 1            |

> **Итог:** Модель не научилась порождать осмысленные ответы — генерации бессвязны, повторяются и не несут информативной нагрузки.

---
